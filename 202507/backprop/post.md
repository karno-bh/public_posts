# Backpropagation for Kids

So, what is it all about? As of writting this text I am in the middle of well-known course abour neural networks. There is a portion of material about RNNs (Recurrent Neural Network). In the homework about RNNs there is a section to do a back propagation just by hands without a framework. I'll give a direct citation: If, however, you are an expert in calculus (or are just curious) and want to see the details of backprop in RNNs, you can work through this optional portion of the notebook. I already saw something like that earlier. This is a citation on the CNN's (Convolution Neural Network) part of the course, when it comes to implementing the backprop without frameworks: The backward pass for convolutional networks is complicated. What's common in that is that feels like you will touch now a hidden mysterious knowledge. On the other hand if you're really curious, you may yourself a question: if it is really complex and you need to be a calculus expert, so how come do frameworks really automate the process? This question is important, since if a framework may automate the process, it should be pretty mechanical one. If it's a mechanical process, _there should be something conceptual in its kernel_. Also, probably, the concept should not be overcomplicated (maybe yes, maybe no, let's take it as an assumption). If you will try to find an information about back propagation on Internet for NNs, there are many articles that start from showing a computation graph. However, (personally) I found them also very hard to understand. First, many of these graph are too oversimplified - it's very hard to understand what author means it the graph has 2-3 . In addition, explanations about it lack the concretenes. And, maybe the most sad things about these explanations, they don't show the code of backprop idea. It's something very ephimeral (again, as by me). On the other hand, if you will try to follow backprop explanation for either Fully Connected layers (FC), or CNN, or RNN layers from books, articles, papers you will see some monstrous formulas. I would argue, that it's very hard to grasp what is going from these sources. **The goal of this text is to explain what back propagation is on a simple but not too much example without being an expert in calculus. As well, there will be a working code for that which maybe revealing how frameworks work.**

## Assumptions

First, the text is not intended for mega-gurus of LLMs. This is not about that. It is for those who just do their first babysteps. Although the back propagation is by today's "AI experts" is considered a kind of deep dive, there is a limit of how much deep you need to go. For instance, the law of partial derivates is not proved here. It's taken as-is. It's also assumed that you know Python in some manner.

## Starting Point

